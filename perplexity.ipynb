{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/evaluate/blob/0ca575d7aa0764ea646dcd5a27cb952e587ce9eb/metrics/perplexity/perplexity.py#L14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"bigcode/gpt_bigcode-santacoder\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL).half().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO Confirm that this is correct, and if necessary, get someone at bigcode to fix it so it defaults to this\n",
    "tokenizer.add_special_tokens({'pad_token': '<|endoftext|>'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Real Thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(inputs):\n",
    "    return tokenizer(inputs, return_tensors=\"pt\", padding=True).to(model.device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "def compute_last_n_perplexity(input_toks, last_n):\n",
    "    \"\"\"\n",
    "      Computes the perplexity of the last n tokens of a result.\n",
    "\n",
    "      Args:\n",
    "        r: The result logits from the model.\n",
    "        last_n: The last n tokens to compute perplexity from.\n",
    "\n",
    "      Returns:\n",
    "        The average peroplexity for each result in the batch over the last n tokens.\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        r = model(**input_toks).logits\n",
    "    shift_logits = r[..., :-1, :].contiguous()\n",
    "    shift_labels = input_toks.input_ids[..., 1:].contiguous()\n",
    "    shift_attention_mask_batch = input_toks.attention_mask[..., 1:].contiguous()\n",
    "\n",
    "    shift_logits = shift_logits[..., :-last_n, :]\n",
    "    shift_labels = shift_labels[..., :-last_n]\n",
    "    shift_attention_mask_batch = shift_attention_mask_batch[..., :-last_n]\n",
    "\n",
    "    perplexity_batch = torch.exp(\n",
    "                    (loss_fct(shift_logits.transpose(1, 2), shift_labels) * shift_attention_mask_batch).sum(1)\n",
    "                    / shift_attention_mask_batch.sum(1)\n",
    "                )\n",
    "\n",
    "    return perplexity_batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strangeness of Perplexity\n",
    "Perplexity seems to change when the initial padding of input changes. Unsure why this is ask Daniel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT1 = \"def foo(n):\\n\\treturn n + 1\"\n",
    "INPUT2 = \"def foo(n: str):\\n\\treturn n + 1\" \n",
    "INPUT3 = \"# s is an integer\\ndef foo(s):\\n\\treturn s + 1\"\n",
    "INPUT4 = \"def foo(s: int):\\n\\treturn s + 1\"\n",
    "INPUT5 = \"def foo(s):\\n\\treturn s + 1\"\n",
    "INPUT6 = \"this is really long input that is not a valid python code. this is really long input that is not a valid python code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.0469, 13.8281], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_last_n_perplexity(get_tokens([INPUT1, INPUT2]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.4141, 11.8984], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_last_n_perplexity(get_tokens([INPUT1, INPUT3]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.7812, 16.1094], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_last_n_perplexity(get_tokens([INPUT1, INPUT6]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
