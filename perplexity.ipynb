{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/evaluate/blob/0ca575d7aa0764ea646dcd5a27cb952e587ce9eb/metrics/perplexity/perplexity.py#L14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from typing import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"bigcode/gpt_bigcode-santacoder\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL).half().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, padding_side=\"left\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Real Thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "def compute_last_n_perplexity(snippets: Iterable, last_n: int, max_length: int=None):\n",
    "    \"\"\"\n",
    "      Computes the perplexity of the last n tokens of a result.\n",
    "\n",
    "      Args:\n",
    "        snippets: The code snippets to compute perplexity over.\n",
    "        last_n: The last n tokens to compute perplexity from.\n",
    "        max_length: The most tokens to feed into the model\n",
    "\n",
    "      Returns:\n",
    "        The average peroplexity for each result in the batch over the last n tokens.\n",
    "    \"\"\"\n",
    "    def compute(snippet):\n",
    "        input_toks = tokenizer(snippet, return_tensors=\"pt\").to(model.device)\n",
    "        if max_length:\n",
    "            input_toks.input_ids = input_toks.input_ids[..., :max_length].contiguous()\n",
    "            input_toks.attention_mask = input_toks.attention_mask[..., :max_length].contiguous()\n",
    "\n",
    "        print(input_toks.input_ids.size())\n",
    "        print(input_toks.attention_mask.size())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            r = model(**input_toks).logits\n",
    "        shift_logits = r[..., :-1, :].contiguous()\n",
    "        shift_labels = input_toks.input_ids[..., 1:].contiguous()\n",
    "        shift_attention_mask_batch = input_toks.attention_mask[..., 1:].contiguous()\n",
    "\n",
    "        shift_logits = shift_logits[..., -last_n:, :].contiguous()\n",
    "        shift_labels = shift_labels[..., -last_n:].contiguous()\n",
    "        shift_attention_mask_batch = shift_attention_mask_batch[..., -last_n:].contiguous()\n",
    "\n",
    "        perplexity_batch = torch.exp(\n",
    "                        (loss_fct(shift_logits.transpose(1, 2), shift_labels) * shift_attention_mask_batch).sum(1)\n",
    "                        / shift_attention_mask_batch.sum(1)\n",
    "                    )\n",
    "\n",
    "        return perplexity_batch\n",
    "    return [compute(snippet).cpu().numpy()[0] for snippet in snippets]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strangeness of Perplexity\n",
    "Perplexity seems to change when the initial padding of input changes. Unsure why this is ask Daniel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT1 = \"def foo(n):\\n\\treturn n + 1\"\n",
    "INPUT2 = \"def foo(n: str):\\n\\treturn n + 1\" \n",
    "INPUT3 = \"# s is an integer\\ndef foo(s):\\n\\treturn s + 1\"\n",
    "INPUT4 = \"def foo(s: int):\\n\\treturn s + 1\"\n",
    "INPUT5 = \"def foo(s):\\n\\treturn s + 1\"\n",
    "INPUT6 = \"this is really long input that is not a valid python code. this is really long input that is not a valid python code\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n",
      "torch.Size([1, 14])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.249, 4.434]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_last_n_perplexity([INPUT1, INPUT2], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n",
      "torch.Size([1, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.249, 1.259]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_last_n_perplexity([INPUT1, INPUT3], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12])\n",
      "torch.Size([1, 25])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.249, 1.019]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_last_n_perplexity([INPUT1, INPUT6], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import itertools\n",
    "\n",
    "d = \"../TypeWhich/src/\"\n",
    "\n",
    "def read_file(p):\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "results = [ read_file(p) for p in itertools.chain(Path(d).glob(\"*.rs\")) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mod benchmark;\\nmod cgen;\\nmod eval;\\nmod grift;\\nmod ins_and_outs;\\nmod insert_coercions;\\nmod parser;\\nmod precision;\\nmod pretty;\\nmod syntax;\\nmod type_check;\\nmod z3_state;\\n\\nuse clap::Clap;\\nuse std::io::*;\\nuse std::path::Path;\\n\\n#[derive(Clap)]\\nenum Parser {\\n    Empty,\\n    Grift,\\n}\\n\\n#[derive(Clone, Copy, PartialEq)]\\nenum Annot {\\n    Ignore,\\n    Hard,\\n}\\n\\nimpl std::str::FromStr for Annot {\\n    type Err = &\\'static str;\\n\\n    fn from_str(s: &str) -> std::result::Result<Self, Self::Err> {\\n        match s {\\n            \"ignore\" => Ok(Annot::Ignore),\\n            \"hard\" => Ok(Annot::Hard),\\n            _ => Err(\"invalid annotation behavior\"),\\n        }\\n    }\\n}\\n\\nimpl std::str::FromStr for Parser {\\n    type Err = &\\'static str;\\n\\n    fn from_str(s: &str) -> std::result::Result<Self, Self::Err> {\\n        match s {\\n            \"grift\" => Ok(Parser::Grift),\\n            \"empty\" => Ok(Parser::Empty),\\n            _ => Err(\"invalid parser\"),\\n        }\\n    }\\n}\\n\\n#[derive(Clap)]\\n#[clap(name = env!(\"CARGO_PKG_NAME\"), version = env!(\"CARGO_PKG_VERSION\"))]\\nstruct TopLevel {\\n    #[clap(subcommand)]\\n    sub_command: SubCommand,\\n}\\n\\n#[derive(Clap)]\\nenum SubCommand {\\n    Migrate(Opts),\\n    Eval(EvalOpts),\\n    Benchmark(BenchmarkOpts),\\n    LatexBenchmarks(BenchmarkOpts),\\n    LatexBenchmarkSummary(BenchmarkOpts),\\n    LatexConciseSummary(BenchmarkOpts),\\n}\\n\\n#[derive(Clap)]\\nstruct EvalOpts {\\n    input: String,\\n    #[clap(short = \\'c\\', long)]\\n    show_inserted_coercions: bool,\\n}\\n\\n#[derive(Clap)]\\nstruct BenchmarkOpts {\\n    input: String,\\n    // Tools to ignore from the benchmark set\\n    #[clap(long)]\\n    ignore: Vec<String>,\\n}\\n\\n#[derive(Clap)]\\npub struct Opts {\\n    /// Input file (defaults to \\'-\\', meaning STDIN)\\n    #[clap(index = 1, default_value = \"-\")]\\n    input: String,\\n    /// Print debugging output\\n    #[clap(short, long)]\\n    debug: bool,\\n    /// Disable the optimizer, which uses \\'assert_soft\\' to reduce the number of\\n    /// coercions.\\n    #[clap(long = \"no-optimize\")]\\n    disable_optimizer: bool,\\n    /// Produce an exact type that may not be safe in all contexts\\n    #[clap(long = \"precise\")]\\n    unsafe_mode: bool,\\n    /// All uses of a variable have the same type (by default, variables can be weakened)\\n    #[clap(long = \"rigid-vars\")]\\n    rigid_variables: bool,\\n    /// Do not type-check the final result of migration\\n    #[clap(long)]\\n    skip_type_check: bool,\\n    // Select the parser\\n    #[clap(short, long, default_value = \"empty\")]\\n    parser: Parser,\\n    /// Use a predefined environment; when \\'-p grift\\' is set, will default to\\n    /// \\'grift\\', otherwise it will be \\'empty\\'\\n    #[clap(\\n        short,\\n        long,\\n        default_value_if(\"parser\", Some(\"grift\"), \"grift\"),\\n        default_value(\"empty\")\\n    )]\\n    env: Parser,\\n    /// Specifies behavior on type annotations; when \\'-p grift\\' is set, will\\n    /// default to \\'ignore\\'.\\n    #[clap(\\n        short,\\n        long,\\n        default_value_if(\"parser\", Some(\"grift\"), \"ignore\"),\\n        default_value(\"ignore\")\\n    )]\\n    annot: Annot,\\n    /// Use ins and outs. Lots of features unsupported in this mode.\\n    #[clap(long)]\\n    ins_and_outs: bool,\\n    /// When a file is provided, inferred types are corresponded to the\\n    /// provided file\\'s type annotations and whether they match (ignoring\\n    /// annotations, coercions, and unannotated identifiers) is printed\\n    #[clap(long)]\\n    compare: Option<String>,\\n}\\n\\n#[derive(Clone, Copy)]\\npub struct Options {\\n    optimizer: bool,\\n    context: bool,\\n    debug: bool,\\n    rigid_vars: bool,\\n    annot: Annot,\\n}\\n\\nimpl Default for Options {\\n    fn default() -> Self {\\n        Options {\\n            optimizer: true,\\n            context: true,\\n            debug: false,\\n            rigid_vars: false,\\n            annot: Annot::Hard,\\n        }\\n    }\\n}\\n\\nfn main() -> Result<()> {\\n    let top_level = TopLevel::parse();\\n    match top_level.sub_command {\\n        SubCommand::Migrate(opts) => migrate_main(opts),\\n        SubCommand::Eval(opts) => eval_main(opts),\\n        SubCommand::Benchmark(opts) => {\\n            benchmark::benchmark_main(&opts.input, opts.ignore.as_slice())\\n        }\\n        SubCommand::LatexBenchmarks(opts) => benchmark::details_latex(&opts.input),\\n        SubCommand::LatexBenchmarkSummary(opts) => benchmark::summarize_latex(&opts.input),\\n        SubCommand::LatexConciseSummary(opts) => benchmark::summarize_latex_concise(&opts.input),\\n    }\\n}\\n\\nfn eval_main(opts: EvalOpts) -> Result<()> {\\n    let src_txt = std::fs::read_to_string(opts.input)?;\\n    let mut src_ast = parser::parse(&src_txt).expect(\"parse error\");\\n    insert_coercions::insert_coercions(&mut src_ast).unwrap();\\n    if opts.show_inserted_coercions {\\n        println!(\"With coercions:\\\\n{}\", &src_ast);\\n    }\\n    match eval::eval(src_ast) {\\n        Ok(_) => println!(\"OK\"),\\n        Err(s) => println!(\"{}\", s),\\n    };\\n    Ok(())\\n}\\n\\nfn language_or_override<\\'a>(language: &\\'a Parser, an_override: &\\'a Parser) -> &\\'a Parser {\\n    match an_override {\\n        Parser::Grift => an_override,\\n        Parser::Empty => language,\\n    }\\n}\\n\\nfn migrate_main(config: Opts) -> Result<()> {\\n    let options = Options {\\n        optimizer: !config.disable_optimizer,\\n        context: !config.unsafe_mode,\\n        debug: config.debug,\\n        rigid_vars: config.rigid_variables,\\n        annot: config.annot,\\n    };\\n\\n    let language = match config.parser {\\n        Parser::Grift => Parser::Grift,\\n        Parser::Empty => match &config.input[..] {\\n            \"-\" => Parser::Empty,\\n            _ => match Path::new(&config.input).extension() {\\n                Some(ext) => match ext.to_str().expect(\"non utf-8\") {\\n                    \"grift\" => Parser::Grift,\\n                    _ => Parser::Empty,\\n                },\\n                None => Parser::Empty,\\n            },\\n        },\\n    };\\n\\n    let env = match language_or_override(&language, &config.env) {\\n        Parser::Grift => grift::env(),\\n        _ => Default::default(),\\n    };\\n    let source = match config.input.as_str() {\\n        \"-\" => {\\n            let mut out = String::new();\\n            stdin().read_to_string(&mut out)?;\\n            out\\n        }\\n        file => std::fs::read_to_string(file)?,\\n    };\\n\\n    let mut parsed = match language {\\n        Parser::Empty => parser::parse(&source).unwrap(),\\n        Parser::Grift => grift::parse(&source),\\n    };\\n\\n    if options.annot == Annot::Ignore {\\n        parsed.fresh_types();\\n    }\\n\\n    if options.debug {\\n        eprintln!(\"Parsed program:\");\\n        eprintln!(\"{}\", parsed);\\n    }\\n    let inferred = if config.ins_and_outs {\\n        parsed.fresh_types();\\n        ins_and_outs::typeinf_portable(parsed)\\n    } else {\\n        cgen::typeinf_options(parsed, &env, options).unwrap()\\n    };\\n\\n    if !config.skip_type_check {\\n        let typ = type_check::tcheck(&env, &inferred)\\n            .map_err(|e| Error::new(ErrorKind::Other, format!(\"{}\", e)))?;\\n        if options.debug {\\n            eprintln!(\"Inferred type:\");\\n            eprintln!(\"{}\", typ);\\n        }\\n    }\\n\\n    match config.compare {\\n        None => {\\n            match language {\\n                Parser::Empty => println!(\"{}\", &inferred),\\n                Parser::Grift => inferred.print_id_types(),\\n            }\\n            Ok(())\\n        }\\n        Some(f) => {\\n            let compare_to_str = std::fs::read_to_string(f)?;\\n            let compare_to = grift::parse(&compare_to_str);\\n            match inferred.matches_roughly(&compare_to) {\\n                Ok(()) => {\\n                    println!(\"MATCHES\");\\n                    Ok(())\\n                }\\n                Err(e) => {\\n                    println!(\"{}\", e);\\n                    std::process::exit(1);\\n                }\\n            }\\n        }\\n    }\\n}\\n\\n#[cfg(test)]\\nmod tests_631 {\\n    use super::cgen::typeinf_options;\\n    use super::parser::parse;\\n    use super::syntax::{Coerce, Exp, Typ};\\n    use super::type_check::type_check;\\n    use super::Options;\\n    trait PairOr {\\n        fn or(&self, other: Self) -> Self;\\n    }\\n    impl PairOr for (bool, bool) {\\n        fn or(&self, other: Self) -> Self {\\n            (self.0 || other.0, self.1 || other.1)\\n        }\\n    }\\n    fn coerce_contains_coercions(c: Coerce) -> (bool, bool) {\\n        match c {\\n            Coerce::Doomed => (false, true),\\n            Coerce::Id => (false, false),\\n            Coerce::Seq(a, b) => coerce_contains_coercions(*a).or(coerce_contains_coercions(*b)),\\n            Coerce::Tag(_) => (true, false),\\n            Coerce::Untag(_) => (false, true),\\n            Coerce::Wrap(..) => panic!(\"wrap shouldn\\'t happen in TypeWhich\"),\\n        }\\n    }\\n    // (to_any, from_any)\\n    pub fn contains_coercions(e: Exp) -> (bool, bool) {\\n        match e {\\n            Exp::PrimCoerce(c, e) => contains_coercions(*e).or(coerce_contains_coercions(c)),\\n            Exp::Coerce(t1, t2, e) => {\\n                let cts = contains_coercions(*e);\\n                if t1 == t2 {\\n                    // this probably shouldn\\'t happen after proper annotation\\n                    cts\\n                } else {\\n                    // a coercion between two non-anys counts as a from_any\\n                    // because it is possibly unsafe (which is what we really\\n                    // mean by from_any)\\n                    (t2 == Typ::Any, t2 != Typ::Any)\\n                }\\n            }\\n            Exp::Lit(..) | Exp::Var(..) | Exp::Empty(..) => (false, false),\\n            Exp::Fun(_, _, e)\\n            | Exp::Fix(_, _, e)\\n            | Exp::Ann(e, _)\\n            | Exp::Fst(e)\\n            | Exp::Snd(e)\\n            | Exp::Head(e)\\n            | Exp::Tail(e)\\n            | Exp::UnaryOp(_, e)\\n            | Exp::Box(e)\\n            | Exp::Unbox(e)\\n            | Exp::IsEmpty(e)\\n            | Exp::IsBool(e)\\n            | Exp::IsInt(e)\\n            | Exp::IsString(e)\\n            | Exp::IsList(e)\\n            | Exp::IsFun(e)\\n            | Exp::VectorLen(e) => contains_coercions(*e),\\n            Exp::App(e1, e2)\\n            | Exp::BinaryOp(_, e1, e2)\\n            | Exp::AddOverload(e1, e2)\\n            | Exp::Cons(e1, e2)\\n            | Exp::Pair(e1, e2)\\n            | Exp::BoxSet(e1, e2)\\n            | Exp::Let(.., e1, e2)\\n            | Exp::Vector(e1, e2)\\n            | Exp::VectorRef(e1, e2) => contains_coercions(*e1).or(contains_coercions(*e2)),\\n            Exp::If(e1, e2, e3) | Exp::VectorSet(e1, e2, e3) => contains_coercions(*e1)\\n                .or(contains_coercions(*e2))\\n                .or(contains_coercions(*e3)),\\n            Exp::LetRec(bindings, e) => bindings\\n                .into_iter()\\n                .fold(contains_coercions(*e), |cc, (_, _, ei)| {\\n                    cc.or(contains_coercions(ei))\\n                }),\\n        }\\n    }\\n    pub fn succeeds(program: &str) -> Typ {\\n        exp_succeeds(parse(program).unwrap())\\n    }\\n    pub fn no_from_any(program: &str) {\\n        let orig = parse(program).unwrap();\\n        let (_, e) = compile_verbose(orig);\\n        let coercions = contains_coercions(e);\\n        assert!(!coercions.1);\\n    }\\n    pub fn coerces(program: &str) -> Typ {\\n        exp_coerces(parse(program).unwrap())\\n    }\\n    fn compile_verbose(mut orig: Exp) -> (Typ, Exp) {\\n        orig.fresh_types();\\n        println!(\"\\\\nOriginal program:\\\\n{}\", &orig);\\n        let mut options = Options::default();\\n        options.debug = true;\\n        let e = typeinf_options(orig, &Default::default(), options).unwrap();\\n        println!(\"\\\\nAfter type inference:\\\\n{}\", e);\\n        let t = type_check(&e).expect(\"failed to typecheck\");\\n        println!(\"\\\\nProgram type:\\\\n{}\", t);\\n        (t, e)\\n    }\\n    pub fn exp_succeeds(orig: Exp) -> Typ {\\n        let (t, e) = compile_verbose(orig);\\n        let coercions = contains_coercions(e);\\n        assert!(!coercions.0 && !coercions.1);\\n        t\\n    }\\n    pub fn exp_coerces(orig: Exp) -> Typ {\\n        let (t, e) = compile_verbose(orig);\\n        let coercions = contains_coercions(e);\\n        assert!(coercions.0 || coercions.1);\\n        t\\n    }\\n    #[test]\\n    fn addition() {\\n        succeeds(\"200 + 9101\");\\n    }\\n    #[test]\\n    fn num_plus_bool() {\\n        coerces(\"1 + true\");\\n    }\\n    /// this isn\\'t really what the 631 test was saying, but it\\'s added here to\\n    /// make sure the above isn\\'t a bug\\n    #[test]\\n    fn num_plus_bool_janky() {\\n        coerces(\"1 +? true\");\\n    }\\n    #[test]\\n    fn indir_int_equal_bool() {\\n        coerces(\\n            \"fun p .\\n                (fun foo . foo 10 p true)\\n                    (fun x . fun y . fun z .\\n                        if true then z\\n                        else\\n                            (fun w . w) (if true then y else (fun w0 . w0) x))\",\\n        );\\n    }\\n    #[test]\\n    fn key_is_bool_and_int() {\\n        // previously ended in else key > 10 which would yield bool and\\n        // constrain key to int, so we make something similar type-wise without\\n        // adding comparisons\\n        coerces(\\n            \"fun key . if true then if true then key else true else\\n                (fun i . true) (key + 10)\",\\n        );\\n    }\\n    #[test]\\n    fn lots_of_conditionals() {\\n        coerces(\\n            \"fun x . fun y . fun z .\\n               (if true then x else y) :: (if true then y else z) ::\\n                  (if true then z else (fun w . w) 5) :: (if x then empty else empty)\",\\n        );\\n    }\\n    #[test]\\n    fn bool_const() {\\n        succeeds(\"true\");\\n    }\\n    #[test]\\n    fn list_of_booleans() {\\n        succeeds(\"true :: empty\");\\n    }\\n    #[test]\\n    fn list_of_numbers() {\\n        succeeds(\"100 :: empty\");\\n    }\\n    #[test]\\n    fn factorial() {\\n        // should be if n == 0 instead of if false but it\\'s probably not a\\n        // particularly important operation to have\\n        succeeds(\\n            \"let fac = fix fac . fun n . if false then 1 else n * fac (n + -1) in\\n             fac 50 + fac 100\",\\n        );\\n    }\\n    #[test]\\n    fn extract_list() {\\n        succeeds(\"head (2 :: empty) + 5\");\\n    }\\n    #[test]\\n    fn identity_polymorphic() {\\n        coerces(\\n            \"let id = fun x . x in\\n            let f = fun anid .\\n                let n = id 10 in\\n                let b = id true in\\n                5 in\\n            f id\",\\n        );\\n    }\\n    #[test]\\n    fn simple_arith() {\\n        succeeds(\"(fun x . x + 1) 10\");\\n    }\\n    #[test]\\n    fn numeric_const() {\\n        succeeds(\"908\");\\n    }\\n    #[test]\\n    fn is_empty_number() {\\n        coerces(\"is_empty 500\");\\n    }\\n    #[test]\\n    fn is_empty_list() {\\n        succeeds(\"is_empty (1 :: empty)\");\\n    }\\n    #[test]\\n    fn real_map() {\\n        succeeds(\\n            \"let map = fix map . fun f . fun lst .\\n               if is_empty(lst) then\\n                 empty\\n               else\\n                 f(head(lst)) :: (map f (tail(lst))) in\\n               map (fun n . n + 1) (1 :: 2 :: 3 :: empty)\",\\n        );\\n    }\\n    #[test]\\n    fn bogus_map() {\\n        succeeds(\\n            \"let map = fun f . fun lst .\\n               f(head(lst)) :: f(head(tail(lst))) :: empty in\\n                   map (fun n . n + 1) (1 :: 2 :: 3 :: empty)\",\\n        );\\n    }\\n    // = not yet supported: extract a value from a record =\\n    // = not yet supported: extract a value from a non-record =\\n    #[test]\\n    fn double() {\\n        succeeds(\\n            \"let square = fun n . if false then 0 else n + n in\\n            square 10 + square 5\",\\n        );\\n    }\\n    #[test]\\n    fn tail_wag() {\\n        succeeds(\"12 :: (tail (12 :: empty))\");\\n    }\\n    #[test]\\n    fn tail_toggle() {\\n        succeeds(\"tail (1 :: empty)\");\\n    }\\n    // = not yet supported: arrays are homogenous =\\n    #[test]\\n    fn dyn_list_single_level() {\\n        coerces(\"1 :: (false :: empty)\");\\n    }\\n    #[test]\\n    fn dyn_list_nested() {\\n        coerces(\"1 :: (false :: ((2 :: (true :: empty)) :: empty))\");\\n    }\\n    #[test]\\n    fn flatten_body() {\\n        coerces(\\n            \"let flatten = fun append . fun f . fun x .\\n               if is_list x then append (f (head x)) (f (tail x)) else x :: empty in\\n               let l = 1 :: (false :: ((2 :: (true :: empty)) :: empty)) in\\n               flatten (fun x . fun y. x) (fun x. x) l\",\\n        );\\n    }\\n}\\n\\n#[cfg(test)]\\nmod tests_migeed_and_parsberg {\\n    use super::cgen::typeinf;\\n    use super::parser::parse;\\n    use super::tests_631::coerces;\\n    use super::type_check::type_check;\\n\\n    // TODO(arjun): _maximal in the name is not accurate. Alternative name:\\n    // assert_ti_ok\\n    fn assert_maximal(program: &str, annotated: &str) {\\n        let mut orig = parse(program).unwrap();\\n        orig.fresh_types();\\n        println!(\"\\\\nOriginal program:\\\\n{}\", &orig);\\n        let e = typeinf(orig).expect(\"type inference failed on the original program\");\\n        println!(\"\\\\nAfter type inference:\\\\n{}\", e);\\n        let correct = typeinf(parse(annotated).unwrap())\\n            .expect(\"type inference failed on the expected program\");\\n        println!(\\n            \"\\\\nProgram type:\\\\n{}\",\\n            type_check(&e).expect(\"failed to typecheck\")\\n        );\\n        println!(\"\\\\nCorrect:\\\\n{}\", correct);\\n        assert_eq!(e, correct);\\n    }\\n    #[test]\\n    #[ignore]\\n    fn apply_add() {\\n        assert_maximal(\"fun x . x (x + 1)\", \"fun x: any . x (x + 1)\");\\n    }\\n\\n    #[test]\\n    #[ignore]\\n    fn add_applied() {\\n        // TODO(arjun): We get a different result. Worth discussing.\\n        assert_maximal(\\n            \"fun x             . x ((x true) + 1)\",\\n            \"fun x: any -> int . x ((x true) + 1)\",\\n        );\\n    }\\n\\n    #[test]\\n    #[ignore]\\n    fn add_two_applies() {\\n        // TODO(arjun): We get a different result. Worth discussing.\\n        assert_maximal(\\n            \"fun x             . x 4 + x true\",\\n            \"fun x: any -> int . x 4 + x true\",\\n        );\\n    }\\n    #[test]\\n    fn identity_four() {\\n        assert_maximal(\"(fun x . x) 4\", \"(fun x: int . x) 4\");\\n    }\\n\\n    #[test]\\n    #[ignore]\\n    fn succ_id_id() {\\n        // TODO(luna): We get a different result, in part because we don\\'t\\n        // allow from_any coercions on arguments\\n        assert_maximal(\\n            \"1 + ((fun y    .y) ((fun x    .x) true))\",\\n            \"1 + ((fun y:int.y) (from_any ((fun x:any.x) true)))\",\\n        );\\n    }\\n    #[test]\\n    fn identity() {\\n        assert_maximal(\"fun x.x\", \"fun x: any . x\");\\n    }\\n\\n    #[test]\\n    #[ignore]\\n    fn apply2() {\\n        // TODO(arjun): We get any -> any -> any as the type on the arrow, which\\n        // results in just as few coercions.\\n        assert_maximal(\\n            \"fun x    .fun y                    .y x x\",\\n            \"fun x:int.fun y:(int -> int -> int).y x x\",\\n        );\\n    }\\n    #[test]\\n    #[ignore]\\n    fn indirect_apply_self() {\\n        // TODO(luna): We get a different result, in part because we don\\'t\\n        // allow from_any coercions on arguments\\n        assert_maximal(\\n            \"fun x    .(fun y    .x)           x  x\",\\n            \"fun x:any.(fun y:int.x) (from_any x) x\",\\n        );\\n    }\\n    #[test]\\n    #[ignore]\\n    fn the_long_one() {\\n        // TODO(luna): We get a different result, in part because we don\\'t\\n        // allow from_any coercions on arguments\\n        assert_maximal(\\n            \"fun x    .(fun f    .(fun x    .fun y    .x)          f (from_any (f x)))(fun z    .1)\",\\n            \"fun x:int.(fun f:any.(fun x:int.fun y:int.x)(from_any f)(from_any (f x)))(fun z:int.1)\",\\n        );\\n    }\\n    /// this benchmark has no maximal migration, which means that x could be\\n    /// given an infinity recursive arrow type (t -> t -> t -> ...). we will\\n    /// give it... something\\n    #[test]\\n    fn apply_self() {\\n        coerces(\"fun x.x x\");\\n    }\\n    /// this benchmark has an unknown maximal migration. because Migeed\\'s\\n    /// algorithm is incomplete, it sometimes does not report whether a maximal\\n    /// solution exists. in practice, this probably means that there is no maximal\\n    /// migration. we still give it some migration\\n    #[test]\\n    fn untypable_in_sys_f() {\\n        coerces(\"(fun x.fun y.y(x(fun x.x))(x(fun b.fun c.b)))(fun d.d d)\");\\n    }\\n    /// unknown to Migeed and Parsberg. self interpreter for the lambda calculus\\n    #[test]\\n    fn self_interpreter() {\\n        coerces(\\n            \"(fun h.(fun x.h(x x))(fun x.h x x))\\n             (fun e.fun m.m(fun x.x)(fun m.fun n.(e m)(e n))(fun m.fun v.e (m v)))\",\\n        );\\n    }\\n}\\n\\n#[cfg(test)]\\nmod tests_misc {\\n    use super::tests_631::coerces;\\n\\n    #[test]\\n    fn fact_church() {\\n        coerces(\\n            \"\\n            let add1  =\\n                  fun x. 1 + x in\\n            let one  =\\n                  fun f. fun x. f x in\\n            let five  =\\n                  fun f. fun x. f (f (f (f (f x)))) in\\n            let pred  =\\n                  fun n.\\n                    (fun f.\\n                      (fun x.\\n                        (((n (fun g. fun h. h (g f)))\\n                          (fun u. x))\\n                         (fun u. u)))) in\\n            let mult  =\\n                  fun m.\\n                    (fun n.\\n                      (fun f. m (n f))) in\\n            let _true   =\\n                   fun a. fun b. a in\\n            let _false  =\\n                   fun a. fun b. b in\\n            let is0   =\\n                  fun n. n (fun x. _false) _true in\\n            let fact  =\\n                  fix fact. fun n.\\n                    ((     (is0 n) // if\\n                           (fun x. one))\\n                           (fun x. (mult n) (fact (pred n)))) in\\n            let realize = fun n . n add1 0 in // : (int -> int) -> (int -> int)\\n            let n = fact five in\\n            realize n\",\\n        );\\n    }\\n    #[test]\\n    fn fact_dyn() {\\n        coerces(\\n            \"\\n            let f = fun f.fun n.\\n                if n = 0\\n                    then 1\\n                    else n * (f f (n + -1)) in\\n            f f 6\",\\n        );\\n    }\\n}\\n'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2000])\n",
      "torch.Size([1, 2000])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2048) must match the size of tensor b (6151) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[94], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m compute_last_n_perplexity(results, \u001b[39m50\u001b[39;49m, max_length\u001b[39m=\u001b[39;49m\u001b[39m2000\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[93], line 39\u001b[0m, in \u001b[0;36mcompute_last_n_perplexity\u001b[0;34m(snippets, last_n, max_length)\u001b[0m\n\u001b[1;32m     33\u001b[0m     perplexity_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(\n\u001b[1;32m     34\u001b[0m                     (loss_fct(shift_logits\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m), shift_labels) \u001b[39m*\u001b[39m shift_attention_mask_batch)\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m                     \u001b[39m/\u001b[39m shift_attention_mask_batch\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m                 )\n\u001b[1;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m perplexity_batch\n\u001b[0;32m---> 39\u001b[0m \u001b[39mreturn\u001b[39;00m [compute(snippet)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m snippet \u001b[39min\u001b[39;00m snippets]\n",
      "Cell \u001b[0;32mIn[93], line 39\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     33\u001b[0m     perplexity_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(\n\u001b[1;32m     34\u001b[0m                     (loss_fct(shift_logits\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m), shift_labels) \u001b[39m*\u001b[39m shift_attention_mask_batch)\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m)\n\u001b[1;32m     35\u001b[0m                     \u001b[39m/\u001b[39m shift_attention_mask_batch\u001b[39m.\u001b[39msum(\u001b[39m1\u001b[39m)\n\u001b[1;32m     36\u001b[0m                 )\n\u001b[1;32m     38\u001b[0m     \u001b[39mreturn\u001b[39;00m perplexity_batch\n\u001b[0;32m---> 39\u001b[0m \u001b[39mreturn\u001b[39;00m [compute(snippet)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m snippet \u001b[39min\u001b[39;00m snippets]\n",
      "Cell \u001b[0;32mIn[93], line 24\u001b[0m, in \u001b[0;36mcompute_last_n_perplexity.<locals>.compute\u001b[0;34m(snippet)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(input_toks\u001b[39m.\u001b[39mattention_mask\u001b[39m.\u001b[39msize())\n\u001b[1;32m     23\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 24\u001b[0m     r \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minput_toks)\u001b[39m.\u001b[39mlogits\n\u001b[1;32m     25\u001b[0m shift_logits \u001b[39m=\u001b[39m r[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mcontiguous()\n\u001b[1;32m     26\u001b[0m shift_labels \u001b[39m=\u001b[39m input_toks\u001b[39m.\u001b[39minput_ids[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, \u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mcontiguous()\n",
      "File \u001b[0;32m~/default/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/default/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:808\u001b[0m, in \u001b[0;36mGPTBigCodeForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[39mlabels (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m    805\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    806\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m--> 808\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    809\u001b[0m     input_ids,\n\u001b[1;32m    810\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    811\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    812\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    813\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    814\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    815\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    816\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    817\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    818\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    819\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    820\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    821\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    822\u001b[0m )\n\u001b[1;32m    823\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    825\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m~/default/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/default/lib/python3.10/site-packages/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py:605\u001b[0m, in \u001b[0;36mGPTBigCodeModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    602\u001b[0m self_attention_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias[\u001b[39mNone\u001b[39;00m, key_length \u001b[39m-\u001b[39m query_length : key_length, :key_length]\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 605\u001b[0m     self_attention_mask \u001b[39m=\u001b[39m self_attention_mask \u001b[39m*\u001b[39;49m attention_mask\u001b[39m.\u001b[39;49mview(batch_size, \u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mto(\n\u001b[1;32m    606\u001b[0m         dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mbool, device\u001b[39m=\u001b[39;49mself_attention_mask\u001b[39m.\u001b[39;49mdevice\n\u001b[1;32m    607\u001b[0m     )\n\u001b[1;32m    609\u001b[0m \u001b[39m# MQA models: (batch_size, query_length, n_heads, key_length)\u001b[39;00m\n\u001b[1;32m    610\u001b[0m \u001b[39m# MHA models: (batch_size, n_heads, query_length, key_length)\u001b[39;00m\n\u001b[1;32m    611\u001b[0m attention_mask \u001b[39m=\u001b[39m self_attention_mask\u001b[39m.\u001b[39munsqueeze(\u001b[39m2\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmulti_query \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2048) must match the size of tensor b (6151) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "compute_last_n_perplexity(results, 50, max_length=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
