{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/huggingface/evaluate/blob/0ca575d7aa0764ea646dcd5a27cb952e587ce9eb/metrics/perplexity/perplexity.py#L14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 812/812 [00:00<00:00, 3.21MB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 2.25G/2.25G [00:32<00:00, 69.0MB/s]\n",
      "/home/claire.schlesinger/default/lib/python3.10/site-packages/torch/cuda/__init__.py:173: UserWarning: \n",
      "NVIDIA H100 PCIe with CUDA capability sm_90 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70 sm_75 sm_80 sm_86.\n",
      "If you want to use the NVIDIA H100 PCIe GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(incompatible_device_warn.format(device_name, capability, \" \".join(arch_list), device_name))\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 159/159 [00:00<00:00, 1.35MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 2.08M/2.08M [00:00<00:00, 24.6MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 138/138 [00:00<00:00, 465kB/s]\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"bigcode/gpt_bigcode-santacoder\"\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL).half().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO Confirm that this is correct, and if necessary, get someone at bigcode to fix it so it defaults to this\n",
    "tokenizer.add_special_tokens({'pad_token': '<|endoftext|>'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
       "         49152, 49152, 49152,   563,  6549,     7,    77,   399,   185,   184,\n",
       "           640,   293,   385,   207,    16],\n",
       "        [49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
       "         49152,   563,  6549,     7,    77,    25,   756,   399,   185,   184,\n",
       "           640,   293,   385,   207,    16],\n",
       "        [49152, 49152, 49152, 49152, 49152, 49152, 49152,     2,   308,   438,\n",
       "           743,  5753,   185,   563,  6549,     7,    82,   399,   185,   184,\n",
       "           640,   308,   385,   207,    16],\n",
       "        [49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
       "         49152,   563,  6549,     7,    82,    25,   646,   399,   185,   184,\n",
       "           640,   308,   385,   207,    16],\n",
       "        [49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152, 49152,\n",
       "         49152, 49152, 49152,   563,  6549,     7,    82,   399,   185,   184,\n",
       "           640,   308,   385,   207,    16],\n",
       "        [  573,   438, 13782,  1859,  1142,   954,   438,   575,   373,  1635,\n",
       "          3918,  1631,    13,   402,   438, 13782,  1859,  1142,   954,   438,\n",
       "           575,   373,  1635,  3918,  1631]], device='cuda:0'), 'attention_mask': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1],\n",
       "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1]], device='cuda:0')}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT1 = \"def foo(n):\\n\\treturn n + 1\"\n",
    "INPUT2 = \"def foo(n: str):\\n\\treturn n + 1\" \n",
    "INPUT3 = \"# s is an integer\\ndef foo(s):\\n\\treturn s + 1\"\n",
    "INPUT4 = \"def foo(s: int):\\n\\treturn s + 1\"\n",
    "INPUT5 = \"def foo(s):\\n\\treturn s + 1\"\n",
    "INPUT6 = \"this is really long input that is not a valid python code. this is really long input that is not a valid python code\"\n",
    "input_toks = tokenizer([INPUT1, INPUT2, INPUT3, INPUT4, INPUT5, INPUT6], return_tensors=\"pt\", padding=True).to(model.device)\n",
    "input_toks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Real Thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "def compute_last_n_perplexity(r, last_n):\n",
    "    \"\"\"\n",
    "      Computes the perplexity of the last n tokens of a result.\n",
    "\n",
    "      Args:\n",
    "        r: The result logits from the model.\n",
    "        last_n: The last n tokens to compute perplexity from.\n",
    "\n",
    "      Returns:\n",
    "        The average peroplexity for each result in the batch over the last n tokens.\n",
    "    \"\"\"\n",
    "    shift_logits = r[..., :-1, :].contiguous()\n",
    "    shift_labels = input_toks.input_ids[..., 1:].contiguous()\n",
    "    shift_attention_mask_batch = input_toks.attention_mask[..., 1:].contiguous()\n",
    "\n",
    "    shift_logits = shift_logits[..., :-last_n, :]\n",
    "    shift_labels = shift_labels[..., :-last_n]\n",
    "    shift_attention_mask_batch = shift_attention_mask_batch[..., :-last_n]\n",
    "\n",
    "    perplexity_batch = torch.exp(\n",
    "                    (loss_fct(shift_logits.transpose(1, 2), shift_labels) * shift_attention_mask_batch).sum(1)\n",
    "                    / shift_attention_mask_batch.sum(1)\n",
    "                )\n",
    "\n",
    "    return perplexity_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    r = model(**input_toks).logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
